# Week 1



## Introduction to Operating Systems



### Overview

How does a program run?

1. The processor **fetches** an instruction from memory
2. It **decodes** it (i.e. figures out which instruction this is)
3. it **executes** it.
4. Reoeat 1 to 3 until the program finally completes.

**Von Neumann model** - a computer architechture

**Operating System (OS)** - OS is in charge of making sure the system operates correctly and efficiently in an easy-to-use manner.

Q: How does OS do it?

A: **Virtualization** - OS takes a physical resource (e.g. processor, memory, a disk) and transoforms it into a more general virtual form of itself. => We sometimes refer to the OS as a **virtual machine**.

Q: So how do we use the features of the virtual machine (e.g. running a program, allocating memory, accessing a file)?

A: **System Calls** - APIs OS provides to allow users to tell the OS what to do => We sometimes say that the OS provides a **standard library** to applications.

OS is sometimes called a **resource manager** where resources refer to CPU, memory, and disk. It handles questions like "if two programs want to run at the same time, which should run?". 



### Virtualizing The CPU

Q: What does virtualizing the CPU mean?

A: Turning a single CPU into a seeminly infinite number of CPUs and thus allowing many programs to seemingly run at once.



### Visualizing Memory

Q: What does virtualizing memory mean?

A: Each process has its own private virtual address space, which the OS somehow maps onto the physical memory of the machine.



### Concurrency

We need to be careful with concurrency (working on many things at once). For example, in multithreading program, we need to be careful when increamenting the same value in different threads. Incrementing a counter takes three steps: copy the counter, increment, and store it back.



### Persistence

To store the data permanentily regardless of the power, we need a **hard drive** or a **solid-state drive (SSD)**.



### History of OS

1. Eearly operating systems only had libraries.
2. The idea of a system call was invented to protect the computer from malicious programs. (User mode/Kernel mode)
3. Multiprogramming for more efficient usage of CPU. (I/O devices are slow, so why not switch to another job and run it for a while?)
4. PC - personal computer





## Operating System Principles



### Layered Structure and Hierachical Decomposition

Hiearchical decomposition is the process of decomposing a system in a top-down fashion.



### Modularity and Functional Encapsulation

When it is possible to understand and use the functions of a component without understanding its internal structure, we call that component a **module** and say that its implementation details are **encapsulated** within that module. A cohesive module is a module such that it only consists of closely related functionalities.



## Monday Lecture



TODO: What's the difference between API and ABI?



## Wenesday Lecture

TODO: Virtual memory and why it's needed



Moore's Law: 2x transistors/Chip Every 1.5 years

# Week 2



## Chapter 4. The Abstraction: The Process



### Overview

We want to run more than one program at once. We want to run a web browser, mail program, a game, a music player, and so forth all at once. OS does this by virtualizing the CPU and creates the illusions that there is a physical CPU for each process. This is known as **time sharing** of the CPU. The implementation of this can be seen as two parts, machanisms and policy. Mechanisms are low-level methods or protocols that implement a functionality, such as how to stop running one program and start running another. Policy is a more high level algorithms for making some kind of decision within the OS. For example, which program should be run when switching a program. This is called **scheduling policy**.



### The Abstraction: A Process

**Process**: A running program. The data of the process sits in memory (**address space**).

**Program Counter (PC)**: tells us which instruction of hte program is curently being executed.



### Process API

- Create
- Destroy
- Wait
- Miscellaneous Control
- Status



### Process Creation: A Little More Detail

1. Load the code and data into memory from disk
2. Allocate run-time stack memory
3. Start the program running at the entry point



### Process State

- Running: Executing instructions
- Ready
- Blocked: Not ready



## Chapter 5. Interlude: Process API

`fork()` system call: create a new process. Return the child PID. run copy of the same program.

`wait()` system call: wait until child process is done

`exec()` system call: Run different program. Instead of creating a new process, it transforms current process to the new one

We can send a **signal** to a process and suspend its normal execution and run a particular piece of code in response to the signal.



TODO: Why is it hard to create a process? (Why do we need fork())



### Key Terms

- Each process has a name; in most systems, that name is a number 

  known as a **process ID** (**PID**). 

- The **fork()** system call is used in UNIX systems to create a new pro- cess. The creator is called the **parent**; the newly created process is called the **child**. As sometimes occurs in real life [J16], the child process is a nearly identical copy of the parent. 

- The **wait()** system call allows a parent to wait for its child to com- plete execution. 

- The **exec()** family of system calls allows a child to break free from its similarity to its parent and execute an entirely new program. 

- A UNIX **shell** commonly uses fork(), wait(), and exec() to launch user commands; the separation of fork and exec enables fea- tures like **input/output redirection**, **pipes**, and other cool features, all without changing anything about the programs being run. 

- Process control is available in the form of **signals**, which can cause jobs to stop, continue, or even terminate. 

- Which processes can be controlled by a particular person is encap- sulated in the notion of a **user**; the operating system allows multiple users onto the system, and ensures users can only control their own processes. 

- A **superuser** can control all processes (and indeed do many other things); this role should be assumed infrequently and with caution for security reasons. 



TODO: Since process is created by forking and OS is a process, does every application start as clone of OS?





## Chapter 6. Mechanism: Limited Direct Execution



**Time Sharing of CPU**: In order to virtualize the CPU, the operating system needs to somehow share the physical CPU among many jobs running seemingly at the same time. The basic idea is simple: run one process for a little while, then run another one, and so forth.



**Direct Execution: **just run the program directly on the CPU







## Chpater 7. Scheduling: Introduction



### Workload Assumption

1. Each job runs for the same amount of time.
2. All jobs arrive at the same time.
3. Once started, each job runs to completion.
4. All jobs only use the CPU (i.e., they perform no I/O)
5. The run-time of each job is known.

**Job**: equivalent to process

We will gradually relax each of these assumptions.



### Scheduling Metrics

```
Turnaround Time = Time Completion - Time Arrival
```



### FIFO

The problem of FIFO scheduling is known as **convoy effect**, where a number of relatively-short potential consumers of a resource get queued behind a heavyweight resource consumer. This scheduling scenario might remind you of a single line at a grocery store and what you feel like when you see the person in front of you with three carts full of provisions and a checkbook out; it’s going to be a while.



### Shortest Job First (SJF)

SJF is a perfect solution to deal with problem of FIFO. But if we relax the assumption that all jobs arrive at the same time, then it will not work well.

![Screen Shot 2019-04-08 at 8.12.14 AM](/Users/one/Desktop/Screen Shot 2019-04-08 at 8.12.14 AM.png)



### Shortest Time-to-Completion First (STCF)

To address the issue of SJF, we will relax the assumption that each job runs to completion.

![Screen Shot 2019-04-08 at 7.46.07 AM](/Users/one/Desktop/Screen Shot 2019-04-08 at 7.46.07 AM.png)



### A New Metric: Response Time

```
Response Time = (Time job arrives in a system) - (first time it is scheduled)
```

While great for turnaround time, this approach is quite bad for response time and interactivity. We are sitting in front of terminal window. We want to see some sort of interaction. Imagine sitting at a terminal, typing, and having to wait 10 seconds to see a response from the system just because some other job got scheduled in front of yours: not too pleasant.





### Round Robin (RR)

The basic idea is simple: instead of running jobs to completion, RR runs a job for a **time slice** (sometimes called a **scheduling quantum**) and then switches to the next job in the run queue. It repeatedly does so until the jobs are
finished. For this reason, RR is sometimes called **time-slicing**. Switching the job is called **context switching**. Generally, a shorter the time slice, the faster the response time. But if it's too short, we can no longer amortize the cost of context switching. So just make it reasonably short enough. Notice RR is great for response time, but awful for turnaround time.

![Screen Shot 2019-04-08 at 8.23.12 AM](/Users/one/Desktop/Screen Shot 2019-04-08 at 8.23.12 AM.png)



### Incorporating I/O

![Screen Shot 2019-04-08 at 8.30.54 AM](/Users/one/Desktop/Screen Shot 2019-04-08 at 8.30.54 AM.png)

Now we treat each chunk of A as different job and perform STCF.

![Screen Shot 2019-04-08 at 8.31.11 AM](/Users/one/Desktop/Screen Shot 2019-04-08 at 8.31.11 AM.png)



So how do we relax the assumption that we know the run-time of each job? We will discuss this in next chapter.





## Chapter 8. Scheduling: The Multi-Level Feedback Queue



### Multi-Level Feedback Queue (MLFQ)

- **Rule 1:** If Priority(A) > Priority(B), A runs (B doesn’t). 
- **Rule 2:** If Priority(A) = Priority(B), A & B run in round-robin fashion using the time slice (quantum length) of the given queue. 
- **Rule 3:** When a job enters the system, it is placed at the highest priority (the topmost queue). 
- **Rule 4:** Once a job uses up its time allotment at a given level (regardless of how many times it has given up the CPU), its priority is reduced (i.e., it moves down one queue). 
- **Rule 5:** After some time period S, move all the jobs in the system to the topmost queue. 



We don't need to know the run-time of the program anymore. We can predict the run time by checking its bahavior in the past (i.e. if it uses up all the time slice given)



## Object Modules, Linkage Editing, Libraries



### Introduction

- A process is an executing instance of a program. 
- A program is one or more files.



- How do our source modules come to be translated into machine language instructions?
- How do they come to be combined with other machine language modules to form complete programs?
- What is the format of a file that contains an executable program, and how does it come to be correctly loaded into memory?



### The Software Generation Tool Chain

- **Source Module**: editable text in some language (e.g. C, assembler, Java) that can be translated into machine language by a compiler or assembler.
- **Relocatable Object Modules**: sets of compiled or assembled instructions created from individual source modules, but which are not yet complete programs.
- **Libraries**: collections of object modules, from which we can fetch functions that are required by (and not contained in) the original source/object modules.
- **Load Modules**: complete programs (usually created by combining numerous object modules) that are ready to be loaded into memory (by the operating system) and executed (by the CPU).



TODO: What's the difference betweeen load modules and executable files?

**Compiler**: Reads source modules and included header files, parses the input language (e.g. C or Java), and infers the intended computations, for which it will generate lower level code. More often than not, that code will be produced in assembly language code rather than machine language. This provides greater flexibility for further processing (e.g. optimization), may make the compiler more portable, and simplifies the compiler by pushing some of the work out to a subsequent phase. There are, however, languages (e.g. Java, Python) whose compilers directly produce a pseudo-machine language that will be executed by a virtual machine or interpreter.

TODO: What does "pseudo-machine language that will be executed by a virtual machine or interpreter" mean?

**Assembler**: Assembly language is much lower level, with each line of code often translating directly to a single machine language instruction or data item. But the assembly language still allows the declaration of variables, the use of macros, and references to externally defined code and data. Developers occasionally write routines directly in assembly language (e.g. when they need specific code that the compiler is incapable of generating).

The output of the assembler is an object module containing mostly machine language code. But, because the output corresponds only to a single input module for the linkage editor:

- some functions (or data items) may not yet be present, and so their addresses will not yet be filled in.
- even locally defined symbols may not have yet been assigned hard in-memory addresses, and so may be expressed as offsets relative to some TBD starting point.

TODO: What's macros?

TODO: What's "even locally defined symbols may not have yet been assigned hard in-memory addresses, and so may be expressed as offsets relative to some TBD starting point."?



**Linkage editor**: The linkage editor reads a specified set of object modules, placing them consecutively into a virtual address space, and noting where (in that virtual address space) each was placed. It also notes unresolved external references (to symbols referenced by, but not defined by the loaded object modules). It then searches a specified set of libraries to find object modules that can satisfy those references, and places them in the evolving virtual address space as well. After locating and placing all of the required (specified and implied) object modules, it finds every reference to a relocatable or external symbol and updates it to reflect the address where the desired code/data was actually placed.The resulting bits represent a program that is ready to be loaded into memory and executed, and they are written out into a new file, called an executable load module.



**Program loader**: The program loader is usually part of the operating system. It examines the information in a load module, creates an appropriate virtual address space, and reads the instructions and initialized data values from the load module into that virtual address space. If the load module includes references to additional (shared) libraries, the program loader finds them and maps them into appropriate places in the virtual address space as well.



TODO: When does the mapping of virutal memory to physical memory happen? 



### Object Module

![Screen Shot 2019-04-10 at 9.44.11 AM](/Users/one/Desktop/CS 111/Notes/Screen Shot 2019-04-10 at 9.44.11 AM.png)



TODO: difference between object module and librareis?

TODO: What's trap?

TODO: Difference between trap handler and signal and kill







## Monday Lecture

### Processes Examples

- opening a browser
- Anything you do with OS is a process



**ABI**: an interface that determine where your programs can be run



TODO: What is ISA?



We write programs as if computer has infinite amount of resources. 



### State

- State machine



### State Example

File: open state or close state



Process is a object with different states.



First thing process does is to craete address space.

A process’ address space is made up of all memory locations that the process can address

If an address isn’t in its address space, the process can’t request access to it. If it wants, it has to ask OS for that.



Different types of memory elements have different requirements

E.g., code is not writable but must be executable

And stacks are readable and writable but not executable



TODO: Difference between stack and heap?





load module: a program that's ready to execute.





Why do we want to load code to memory? - To run the instructions faster. 





TODO: Mutex



TODO: What's processor core



**Policy**: High level description describing the algorithm

**Mechanism**: The actual implementation of the algorithm



## Wednesday Lecture

Context switching is expensive



Midterm?

- Time Sharing
- Batch
- Real-Time



Non-determinisitic timing: Context Switching

# Week 3



## Chapter 13. The Abstraction: Address Spaces



### Overview

We have seen the introduction of a major OS subsystem: virtual memory. The VM system is responsible for providing the illusion of a large, sparse, private address space to programs, which hold all of their instructions and data therein. The OS, with some serious hardware help, will take each of these virtual memory references, and turn them into physical addresses, which can be presented to the physical memory in order to fetch the desired information. The OS will do this for many processes at once, making sure to protect programs from one another, as well as pro-
tect the OS. The entire approach requires a great deal of mechanism (lots of low-level machinery) as well as some critical policies to work; we’ll start from the bottom up, describing the critical mechanisms first. And thus we proceed!



**Address Space**: A range of valid addresses in memory that are available for a program or process. The address space of a process contains all of the memory state of the running program.



**Virtual Memory**: When, for example, process A tries to perform a load at address 0 (which we will call a **virtual address**), somehow the OS, in tandem with some hardware support, will have to make sure the load
doesn’t actually go to physical address 0 but rather to physical address 320KB (where A is loaded into memory).



### Goals

- **transparency**: The OS should implement virtual memory in a way that is invisible to the running program. Thus, the program shouldn’t be aware of the fact that memory is virtualized; rather, the program behaves as if it has its own private physical memory. Behind the scenes, the OS (and hardware) does all the work to multiplex memory among many different jobs, and hence implements the illusion.
- **efficiency**: The OS should strive to make the virtualization as **efficient** as possible, both in terms of time (i.e., not making programs run much more slowly) and space (i.e., not using too much memory for structures needed to support virtualization).
- **protection**: The OS should make sure to **protect** processes from one another as well as the OS itself from pro-
  cesses. When one process performs a load, a store, or an instruction fetch, it should not be able to access or affect in any way the memory contents of any other process or the OS itself (that is, anything *outside* its address
  space).





## Chapter 17. Free-Space Management



### Basic Strategies

- **Best Fit**: The best fit strategy is quite simple: first, search through the free list and
  find chunks of free memory that are as big or bigger than the requested
  size. Then, return the one that is the smallest in that group of candidates.
- **Worst Fit**: The worst fit approach is the opposite of best fit
- **First Fit**
- **Next Fit**
- **Segragated Lists**
- **Buddy Allocation**





## Chapter 15. Mechanism: Address Translation



**Limited Direct Execution**: The idea behind LDE is simple: for the most part, let the program run directly on the hardware; however, at certain key points in time (such as when a process issues a system call, or a timer interrupt occurs), arrange so that the OS gets involved and makes sure the “right” thing happens.



### Assumption

- the user’s address space must be placed *contiguously* in physical memory
- the size of the address space is not too big; specifically, that it is *less than the size of physical memory*
- each address space is exactly the *same size*

We will relax these assumptions later on.



### Base and Bounds Approach



**Base**: each program is written and compiled as if it is loaded at address zero. However, when a program starts running, the OS decides where in physical memory it should be loaded and sets the base register to that value.

```
physical address = virtual address + base
```

Because this relocation of the address happens at runtime, and because we can move address spaces even after the process has started running, the technique is often referred to as **dynamic relocation**.



**Bounds**: Bounds register will be set to the size of the address space of the process. If a process generates a vir-
tual address that is greater than the bounds, or one that is negative, the CPU will raise an exception, and the process will likely be terminated. The point of the bounds is thus to make sure that all addresses generated by the process are legal and within the “bounds” of the process.





Sometimes people call the part of the processor that helps with address translation the **memory management unit (MMU)**









## Monday Lecture











